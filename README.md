# Interesting NLP papers  
## Language Modeling
### Recurrent Neural Network based models
- An Empirical Exploration of Recurrent Network Architectures (Jozefowicz et al) [[pdf](http://proceedings.mlr.press/v37/jozefowicz15.pdf)] [github] [summary]
- Regularizing and Optimizing LSTM Language Models (S.Merity et al) [[pdf](https://arxiv.org/pdf/1708.02182.pdf)] [github] [summary]
- Improving Language Modeling using Densely Connected Recurrent Neural Networks (Godin et al) [[pdf](https://www.aclweb.org/anthology/W17-2622.pdf)] [github] [summary]
- Grow and Prune Compact, Fast, and Accurate LSTMs (Dai et al) [[pdf](https://arxiv.org/pdf/1805.11797.pdf)] [github] [summary]


### Regularization Techniques  
- Recurrent Neural Network Regularization (Zaremba et el) [[pdf](https://arxiv.org/pdf/1409.2329.pdf)] [github] [summary]
- Regularizing and Optimizing LSTM Language Models (S.Merity et al) [[pdf](https://arxiv.org/pdf/1708.02182.pdf)] [github] [summary]
- A Theoretically Grounded Application of Dropout in Recurrent Neural Networks (Gal et al.) [pdf](https://arxiv.org/pdf/1512.05287.pdf)[github][summary]

### Transformer based Language Models
- Generalization Through Memorization: Nearest Neighbor Language Models (Khandelwal et al) (2020) [[pdf](https://arxiv.org/pdf/1911.00172v2.pdf)] [github] [summary]



